{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ceb706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_info(data):\n",
    "    \"\"\"\n",
    "    Parse the API response data (assumed to be JSON) and extract book metadata.\n",
    "\n",
    "    Args:\n",
    "        data (str): JSON string from the API response.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing book metadata.\n",
    "    \"\"\"\n",
    "    books = json.loads(data)\n",
    "\n",
    "    if isinstance(books, dict) and 'results' in books:\n",
    "        books = books['results']\n",
    "\n",
    "    book_info = []\n",
    "    for book in books: \n",
    "        book_id = book.get('id')\n",
    "        title = book.get('title')\n",
    "        authors = [author.get('name') for author in book.get('authors', [])]\n",
    "        subjects = book.get('subjects', [])\n",
    "        bookshelves = book.get('bookshelves', [])\n",
    "        book_info.append({\n",
    "            'id': book_id,\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'subjects': subjects,\n",
    "            'bookshelves': bookshelves\n",
    "        })\n",
    "    return book_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd05d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_creation(book_info, headers, stop_words):\n",
    "    \"\"\"\n",
    "    For each book in book_info, fetch the text, print a sample, and build an inverted index (excluding stop words).\n",
    "\n",
    "    Args:\n",
    "        book_info (list): List of book metadata dictionaries.\n",
    "        headers (dict): Headers for the API request.\n",
    "        stop_words (set): Set of stop words to exclude from the index.\n",
    "\n",
    "    Returns:\n",
    "        dict: Inverted index mapping words to lists of book IDs.\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for book in book_info:\n",
    "        book_id = book['id']\n",
    "        response = requests.get(\n",
    "            f\"https://project-gutenberg-free-books-api1.p.rapidapi.com/books/{book_id}/text?cleaning_mode=simple\", \n",
    "            headers=headers\n",
    "        )\n",
    "        text = response.text\n",
    "        books = json.loads(text)\n",
    "\n",
    "        if isinstance(books, dict) and 'text' in books:\n",
    "            text = books['text']\n",
    "\n",
    "        for word in text.split():\n",
    "            word = word.lower().strip('.,!?;\"()[]{}')\n",
    "            if word and word not in stop_words:\n",
    "                inverted_index[word][book_id] += 1\n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9f0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fetch_and_store_books(data, headers, datalake_dir='datalake'):\n",
    "    \"\"\"\n",
    "    Extract book metadata, fetch book texts, and store metadata in a structured datalake directory.\n",
    "\n",
    "    Args:\n",
    "        data (str): JSON string from the API response.\n",
    "        headers (dict): Headers for the API request.\n",
    "        datalake_dir (str): Root directory for storing data.\n",
    "    Returns:\n",
    "        list: List of book metadata dictionaries.\n",
    "    \"\"\"\n",
    "    book_info = extract_book_info(data)\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_dir = now.strftime('%Y-%m-%d')\n",
    "    time_dir = now.strftime('%H')\n",
    "    full_path = os.path.join(datalake_dir, date_dir, time_dir)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    control_dir = 'control'\n",
    "    downloaded_books_path = os.path.join(control_dir, 'downloaded_books.txt')\n",
    "    indexed_books_path = os.path.join(control_dir, 'indexed_books.txt')\n",
    "\n",
    "    downloaded_books = set()\n",
    "    if os.path.exists(downloaded_books_path):\n",
    "        with open(downloaded_books_path, 'r', encoding='utf-8') as f:\n",
    "            downloaded_books = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    indexed_books = set()\n",
    "    if os.path.exists(indexed_books_path):\n",
    "        with open(indexed_books_path, 'r', encoding='utf-8') as f:\n",
    "            indexed_books = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    for book in book_info:\n",
    "        book_id = str(book['id'])\n",
    "        if book_id in downloaded_books and book_id in indexed_books:\n",
    "            continue \n",
    "\n",
    "        header_path = os.path.join(full_path, f\"{book_id}.header.txt\")\n",
    "        with open(header_path, 'w', encoding='utf-8') as header_file:\n",
    "            json.dump(book, header_file, ensure_ascii=False, indent=4)\n",
    "        response = requests.get(f\"https://project-gutenberg-free-books-api1.p.rapidapi.com/books/{book_id}/text?cleaning_mode=simple\", headers=headers)\n",
    "        text = response.text\n",
    "        book_text = json.loads(text).get('text', '')\n",
    "        body_path = os.path.join(full_path, f\"{book_id}.body.txt\")\n",
    "        with open(body_path, 'w', encoding='utf-8') as body_file:\n",
    "            body_file.write(book_text)\n",
    "\n",
    "    print(\"Books downloaded and stored in datalake.\")\n",
    "    return book_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639e9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datamart_fill(book_info, headers, stop_words):\n",
    "    \"\"\"\n",
    "    This function creates a directory named 'datamart', if it doesn't already exist, \n",
    "    and processes the data stored in the 'datalake' directory. Inside datamart create two files,\n",
    "    unless they already exist: metadata.sql and inverted_index.json\n",
    "\n",
    "    Args:\n",
    "        book_info (list): List of book metadata dictionaries.\n",
    "        headers (dict): Headers for the API request.\n",
    "        stop_words (set): Set of stop words to exclude from the index.\n",
    "    \"\"\"\n",
    "    datamart_dir = 'datamart'\n",
    "    os.makedirs(datamart_dir, exist_ok=True)\n",
    "\n",
    "    metadata_path = os.path.join(datamart_dir, 'metadata.sql')\n",
    "    inverted_index_path = os.path.join(datamart_dir, 'inverted_index.json')\n",
    "    inverted_index = inverted_index_creation(book_info, headers, stop_words)\n",
    "    \n",
    "    datalake_dir = 'datalake'\n",
    "    metadata = []\n",
    "\n",
    "    for root, _, files in os.walk(datalake_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.header.txt'):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    book_metadata = json.load(f)\n",
    "                    metadata.append(book_metadata)\n",
    "\n",
    "    if not os.path.exists(inverted_index_path):\n",
    "        with open(inverted_index_path, 'w', encoding='utf-8') as index_file:\n",
    "            json.dump(inverted_index, index_file, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        with open(inverted_index_path, 'r+', encoding='utf-8') as index_file:\n",
    "            existing_index = json.load(index_file)\n",
    "            existing_index.update(inverted_index) \n",
    "            index_file.seek(0)\n",
    "            json.dump(existing_index, index_file, ensure_ascii=False, indent=4)\n",
    "            index_file.truncate()\n",
    "    \n",
    "    if not os.path.exists(metadata_path):\n",
    "        db_path = os.path.join(datamart_dir, 'metadata.db')\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS books (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            authors TEXT,\n",
    "            subjects TEXT,\n",
    "            bookshelves TEXT\n",
    "            );\n",
    "            \"\"\"\n",
    "        )\n",
    "        for book in metadata:\n",
    "            authors = ', '.join(book.get('authors', []))\n",
    "            subjects = ', '.join(book.get('subjects', []))\n",
    "            bookshelves = ', '.join(book.get('bookshelves', []))\n",
    "            title = book.get('title', '')\n",
    "            cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR IGNORE INTO books (id, title, authors, subjects, bookshelves)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (book.get('id'), title, authors, subjects, bookshelves)\n",
    "            )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    print(\"Datamart filled with metadata and inverted index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233cbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(data):\n",
    "    \"\"\"\n",
    "    Controller function to control which books are downloaded \n",
    "    and which ones are indexed, creating in the directory control\n",
    "    the files downloaded_books.txt and indexed_books.txt.\n",
    "    \"\"\"\n",
    "\n",
    "    control_dir = 'control'\n",
    "    os.makedirs(control_dir, exist_ok=True)\n",
    "    downloaded_books_path = os.path.join(control_dir, 'downloaded_books.txt')\n",
    "    indexed_books_path = os.path.join(control_dir, 'indexed_books.txt')\n",
    "\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"29ab1edf9dmshb37d07ffbb17e29p1ce99ejsn7592f187c027\",\n",
    "        'x-rapidapi-host': \"project-gutenberg-free-books-api1.p.rapidapi.com\"\n",
    "    }\n",
    "        \n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    book_info = extract_fetch_and_store_books(data.decode(\"utf-8\"), headers)\n",
    "    \n",
    "    with open(downloaded_books_path, 'w', encoding='utf-8') as f:\n",
    "        for book in book_info:\n",
    "            f.write(f\"{book['id']}\\n\")\n",
    "\n",
    "    datamart_fill(book_info, headers, stop_words)\n",
    "\n",
    "    with open(indexed_books_path, 'w', encoding='utf-8') as f:\n",
    "        for book in book_info:\n",
    "            f.write(f\"{book['id']}\\n\")\n",
    "\n",
    "    print(\"Controller finished processing books.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e680ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url = \"https://project-gutenberg-free-books-api1.p.rapidapi.com/books\"):\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"f38981c70cmsh4f5bf253e75dad5p1b6022jsn0f1975492054\",\n",
    "        'x-rapidapi-host': \"project-gutenberg-free-books-api1.p.rapidapi.com\"\n",
    "        }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.content)\n",
    "    controller(response.content)\n",
    "\n",
    "    print(\"Page processed, moving to next page if available...\")\n",
    "    url = response.json().get('next')\n",
    "    if url == \"https://project-gutenberg-free-books-api1.p.rapidapi.com/books?page=6\":\n",
    "        return \"First 20 pages done\"\n",
    "    else:\n",
    "        main(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93338d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"message\":\"You have exceeded the MONTHLY quota for Requests on your current plan, BASIC. Upgrade your plan at https:\\\\/\\\\/rapidapi.com\\\\/help-lQ_hVT8W5\\\\/api\\\\/project-gutenberg-free-books-api1\"}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcubt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcontroller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage processed, moving to next page if available...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mcontroller\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     18\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m book_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_fetch_and_store_books\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(downloaded_books_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m book_info:\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mextract_fetch_and_store_books\u001b[1;34m(data, headers, datalake_dir)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_fetch_and_store_books\u001b[39m(data, headers, datalake_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatalake\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Extract book metadata, fetch book texts, and store metadata in a structured datalake directory.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        list: List of book metadata dictionaries.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     book_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_book_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     15\u001b[0m     date_dir \u001b[38;5;241m=\u001b[39m now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mextract_book_info\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     16\u001b[0m book_info \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m books: \n\u001b[1;32m---> 18\u001b[0m     book_id \u001b[38;5;241m=\u001b[39m \u001b[43mbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     title \u001b[38;5;241m=\u001b[39m book\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m     authors \u001b[38;5;241m=\u001b[39m [author\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m book\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m, [])]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67979, 'The Blue Castle: a novel', 'Montgomery, L. M. (Lucy Maud)')\n"
     ]
    }
   ],
   "source": [
    "db_path = \"datamart/metadata.db\"\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT id, title, authors\n",
    "    FROM books\n",
    "    WHERE subjects LIKE '%Romance%'\n",
    "    ORDER BY title ASC\n",
    "\"\"\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
